{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
    "\n",
    "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import string\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu device is available\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('{} device is available'.format(device))\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPenWOy01Ooa",
    "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
   },
   "source": [
    "#### 1. Загрузка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __________start of block__________\n",
    "#!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
    "\n",
    "with open('onegin.txt', 'r', encoding='utf-8') as iofile:  # указываем кодировку utf-8\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
    "# __________end of block__________\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQYpmGfR_gJ8"
   },
   "source": [
    "#### 2. Построение словаря и предобработка текста\n",
    "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "tokens = sorted(set(text.lower())) + ['<sos>']\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "assert num_tokens == 84, \"Check the tokenization process\"\n",
    "\n",
    "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
    "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
    "\n",
    "print(\"Seems fine!\")\n",
    "\n",
    "\n",
    "text_encoded = [token_to_idx[x] for x in text]\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
    "\n",
    "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
    "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
    "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
    "\n",
    "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
    "\n",
    "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "batch_size = 256\n",
    "seq_length = 500\n",
    "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
    "\n",
    "def generate_chunk():\n",
    "    global text_encoded, start_column, batch_size, seq_length\n",
    "\n",
    "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
    "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
    "    yield np.hstack((start_column, data))\n",
    "# __________end of block__________    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример батча:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(generate_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 501)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вам предстоит написать код для обучения модели и генерации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input)  # (batch_size, seq_length, hidden_size)\n",
    "        output, hidden = self.gru(input, hidden)  # (batch_size, seq_length, hidden_size)\n",
    "        output = self.decoder(output.reshape(-1, self.hidden_size))  # Преобразуем для Linear слоя\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.0173\n",
      "Epoch [2/1000], Loss: 0.0171\n",
      "Epoch [3/1000], Loss: 0.0169\n",
      "Epoch [4/1000], Loss: 0.0166\n",
      "Epoch [5/1000], Loss: 0.0163\n",
      "Epoch [6/1000], Loss: 0.0158\n",
      "Epoch [7/1000], Loss: 0.0152\n",
      "Epoch [8/1000], Loss: 0.0146\n",
      "Epoch [9/1000], Loss: 0.0141\n",
      "Epoch [10/1000], Loss: 0.0138\n",
      "Epoch [11/1000], Loss: 0.0136\n",
      "Epoch [12/1000], Loss: 0.0135\n",
      "Epoch [13/1000], Loss: 0.0133\n",
      "Epoch [14/1000], Loss: 0.0132\n",
      "Epoch [15/1000], Loss: 0.0132\n",
      "Epoch [16/1000], Loss: 0.0131\n",
      "Epoch [17/1000], Loss: 0.0131\n",
      "Epoch [18/1000], Loss: 0.0131\n",
      "Epoch [19/1000], Loss: 0.0131\n",
      "Epoch [20/1000], Loss: 0.0130\n",
      "Epoch [21/1000], Loss: 0.0130\n",
      "Epoch [22/1000], Loss: 0.0130\n",
      "Epoch [23/1000], Loss: 0.0130\n",
      "Epoch [24/1000], Loss: 0.0130\n",
      "Epoch [25/1000], Loss: 0.0130\n",
      "Epoch [26/1000], Loss: 0.0130\n",
      "Epoch [27/1000], Loss: 0.0130\n",
      "Epoch [28/1000], Loss: 0.0130\n",
      "Epoch [29/1000], Loss: 0.0129\n",
      "Epoch [30/1000], Loss: 0.0129\n",
      "Epoch [31/1000], Loss: 0.0130\n",
      "Epoch [32/1000], Loss: 0.0129\n",
      "Epoch [33/1000], Loss: 0.0129\n",
      "Epoch [34/1000], Loss: 0.0129\n",
      "Epoch [35/1000], Loss: 0.0129\n",
      "Epoch [36/1000], Loss: 0.0129\n",
      "Epoch [37/1000], Loss: 0.0129\n",
      "Epoch [38/1000], Loss: 0.0129\n",
      "Epoch [39/1000], Loss: 0.0129\n",
      "Epoch [40/1000], Loss: 0.0129\n",
      "Epoch [41/1000], Loss: 0.0129\n",
      "Epoch [42/1000], Loss: 0.0129\n",
      "Epoch [43/1000], Loss: 0.0128\n",
      "Epoch [44/1000], Loss: 0.0128\n",
      "Epoch [45/1000], Loss: 0.0128\n",
      "Epoch [46/1000], Loss: 0.0128\n",
      "Epoch [47/1000], Loss: 0.0128\n",
      "Epoch [48/1000], Loss: 0.0128\n",
      "Epoch [49/1000], Loss: 0.0128\n",
      "Epoch [50/1000], Loss: 0.0127\n",
      "Epoch [51/1000], Loss: 0.0127\n",
      "Epoch [52/1000], Loss: 0.0127\n",
      "Epoch [53/1000], Loss: 0.0127\n",
      "Epoch [54/1000], Loss: 0.0126\n",
      "Epoch [55/1000], Loss: 0.0126\n",
      "Epoch [56/1000], Loss: 0.0126\n",
      "Epoch [57/1000], Loss: 0.0125\n",
      "Epoch [58/1000], Loss: 0.0125\n",
      "Epoch [59/1000], Loss: 0.0125\n",
      "Epoch [60/1000], Loss: 0.0125\n",
      "Epoch [61/1000], Loss: 0.0124\n",
      "Epoch [62/1000], Loss: 0.0124\n",
      "Epoch [63/1000], Loss: 0.0123\n",
      "Epoch [64/1000], Loss: 0.0123\n",
      "Epoch [65/1000], Loss: 0.0123\n",
      "Epoch [66/1000], Loss: 0.0123\n",
      "Epoch [67/1000], Loss: 0.0122\n",
      "Epoch [68/1000], Loss: 0.0122\n",
      "Epoch [69/1000], Loss: 0.0122\n",
      "Epoch [70/1000], Loss: 0.0121\n",
      "Epoch [71/1000], Loss: 0.0121\n",
      "Epoch [72/1000], Loss: 0.0121\n",
      "Epoch [73/1000], Loss: 0.0120\n",
      "Epoch [74/1000], Loss: 0.0120\n",
      "Epoch [75/1000], Loss: 0.0120\n",
      "Epoch [76/1000], Loss: 0.0119\n",
      "Epoch [77/1000], Loss: 0.0119\n",
      "Epoch [78/1000], Loss: 0.0119\n",
      "Epoch [79/1000], Loss: 0.0118\n",
      "Epoch [80/1000], Loss: 0.0118\n",
      "Epoch [81/1000], Loss: 0.0117\n",
      "Epoch [82/1000], Loss: 0.0117\n",
      "Epoch [83/1000], Loss: 0.0117\n",
      "Epoch [84/1000], Loss: 0.0116\n",
      "Epoch [85/1000], Loss: 0.0116\n",
      "Epoch [86/1000], Loss: 0.0115\n",
      "Epoch [87/1000], Loss: 0.0115\n",
      "Epoch [88/1000], Loss: 0.0115\n",
      "Epoch [89/1000], Loss: 0.0114\n",
      "Epoch [90/1000], Loss: 0.0114\n",
      "Epoch [91/1000], Loss: 0.0113\n",
      "Epoch [92/1000], Loss: 0.0113\n",
      "Epoch [93/1000], Loss: 0.0113\n",
      "Epoch [94/1000], Loss: 0.0112\n",
      "Epoch [95/1000], Loss: 0.0112\n",
      "Epoch [96/1000], Loss: 0.0112\n",
      "Epoch [97/1000], Loss: 0.0111\n",
      "Epoch [98/1000], Loss: 0.0111\n",
      "Epoch [99/1000], Loss: 0.0110\n",
      "Epoch [100/1000], Loss: 0.0110\n",
      "Epoch [101/1000], Loss: 0.0110\n",
      "Epoch [102/1000], Loss: 0.0109\n",
      "Epoch [103/1000], Loss: 0.0109\n",
      "Epoch [104/1000], Loss: 0.0108\n",
      "Epoch [105/1000], Loss: 0.0108\n",
      "Epoch [106/1000], Loss: 0.0108\n",
      "Epoch [107/1000], Loss: 0.0107\n",
      "Epoch [108/1000], Loss: 0.0107\n",
      "Epoch [109/1000], Loss: 0.0107\n",
      "Epoch [110/1000], Loss: 0.0106\n",
      "Epoch [111/1000], Loss: 0.0106\n",
      "Epoch [112/1000], Loss: 0.0106\n",
      "Epoch [113/1000], Loss: 0.0106\n",
      "Epoch [114/1000], Loss: 0.0105\n",
      "Epoch [115/1000], Loss: 0.0105\n",
      "Epoch [116/1000], Loss: 0.0105\n",
      "Epoch [117/1000], Loss: 0.0105\n",
      "Epoch [118/1000], Loss: 0.0104\n",
      "Epoch [119/1000], Loss: 0.0104\n",
      "Epoch [120/1000], Loss: 0.0104\n",
      "Epoch [121/1000], Loss: 0.0104\n",
      "Epoch [122/1000], Loss: 0.0103\n",
      "Epoch [123/1000], Loss: 0.0103\n",
      "Epoch [124/1000], Loss: 0.0103\n",
      "Epoch [125/1000], Loss: 0.0103\n",
      "Epoch [126/1000], Loss: 0.0103\n",
      "Epoch [127/1000], Loss: 0.0102\n",
      "Epoch [128/1000], Loss: 0.0102\n",
      "Epoch [129/1000], Loss: 0.0102\n",
      "Epoch [130/1000], Loss: 0.0102\n",
      "Epoch [131/1000], Loss: 0.0102\n",
      "Epoch [132/1000], Loss: 0.0101\n",
      "Epoch [133/1000], Loss: 0.0101\n",
      "Epoch [134/1000], Loss: 0.0101\n",
      "Epoch [135/1000], Loss: 0.0101\n",
      "Epoch [136/1000], Loss: 0.0101\n",
      "Epoch [137/1000], Loss: 0.0100\n",
      "Epoch [138/1000], Loss: 0.0100\n",
      "Epoch [139/1000], Loss: 0.0100\n",
      "Epoch [140/1000], Loss: 0.0100\n",
      "Epoch [141/1000], Loss: 0.0100\n",
      "Epoch [142/1000], Loss: 0.0099\n",
      "Epoch [143/1000], Loss: 0.0099\n",
      "Epoch [144/1000], Loss: 0.0099\n",
      "Epoch [145/1000], Loss: 0.0099\n",
      "Epoch [146/1000], Loss: 0.0099\n",
      "Epoch [147/1000], Loss: 0.0098\n",
      "Epoch [148/1000], Loss: 0.0098\n",
      "Epoch [149/1000], Loss: 0.0098\n",
      "Epoch [150/1000], Loss: 0.0098\n",
      "Epoch [151/1000], Loss: 0.0098\n",
      "Epoch [152/1000], Loss: 0.0097\n",
      "Epoch [153/1000], Loss: 0.0097\n",
      "Epoch [154/1000], Loss: 0.0097\n",
      "Epoch [155/1000], Loss: 0.0097\n",
      "Epoch [156/1000], Loss: 0.0097\n",
      "Epoch [157/1000], Loss: 0.0096\n",
      "Epoch [158/1000], Loss: 0.0096\n",
      "Epoch [159/1000], Loss: 0.0096\n",
      "Epoch [160/1000], Loss: 0.0096\n",
      "Epoch [161/1000], Loss: 0.0096\n",
      "Epoch [162/1000], Loss: 0.0096\n",
      "Epoch [163/1000], Loss: 0.0095\n",
      "Epoch [164/1000], Loss: 0.0095\n",
      "Epoch [165/1000], Loss: 0.0095\n",
      "Epoch [166/1000], Loss: 0.0095\n",
      "Epoch [167/1000], Loss: 0.0095\n",
      "Epoch [168/1000], Loss: 0.0095\n",
      "Epoch [169/1000], Loss: 0.0094\n",
      "Epoch [170/1000], Loss: 0.0094\n",
      "Epoch [171/1000], Loss: 0.0094\n",
      "Epoch [172/1000], Loss: 0.0094\n",
      "Epoch [173/1000], Loss: 0.0094\n",
      "Epoch [174/1000], Loss: 0.0093\n",
      "Epoch [175/1000], Loss: 0.0093\n",
      "Epoch [176/1000], Loss: 0.0093\n",
      "Epoch [177/1000], Loss: 0.0093\n",
      "Epoch [178/1000], Loss: 0.0093\n",
      "Epoch [179/1000], Loss: 0.0093\n",
      "Epoch [180/1000], Loss: 0.0092\n",
      "Epoch [181/1000], Loss: 0.0092\n",
      "Epoch [182/1000], Loss: 0.0092\n",
      "Epoch [183/1000], Loss: 0.0092\n",
      "Epoch [184/1000], Loss: 0.0092\n",
      "Epoch [185/1000], Loss: 0.0091\n",
      "Epoch [186/1000], Loss: 0.0091\n",
      "Epoch [187/1000], Loss: 0.0091\n",
      "Epoch [188/1000], Loss: 0.0091\n",
      "Epoch [189/1000], Loss: 0.0091\n",
      "Epoch [190/1000], Loss: 0.0090\n",
      "Epoch [191/1000], Loss: 0.0090\n",
      "Epoch [192/1000], Loss: 0.0090\n",
      "Epoch [193/1000], Loss: 0.0090\n",
      "Epoch [194/1000], Loss: 0.0090\n",
      "Epoch [195/1000], Loss: 0.0089\n",
      "Epoch [196/1000], Loss: 0.0089\n",
      "Epoch [197/1000], Loss: 0.0089\n",
      "Epoch [198/1000], Loss: 0.0089\n",
      "Epoch [199/1000], Loss: 0.0089\n",
      "Epoch [200/1000], Loss: 0.0089\n",
      "Epoch [201/1000], Loss: 0.0089\n",
      "Epoch [202/1000], Loss: 0.0088\n",
      "Epoch [203/1000], Loss: 0.0088\n",
      "Epoch [204/1000], Loss: 0.0088\n",
      "Epoch [205/1000], Loss: 0.0088\n",
      "Epoch [206/1000], Loss: 0.0088\n",
      "Epoch [207/1000], Loss: 0.0087\n",
      "Epoch [208/1000], Loss: 0.0087\n",
      "Epoch [209/1000], Loss: 0.0087\n",
      "Epoch [210/1000], Loss: 0.0087\n",
      "Epoch [211/1000], Loss: 0.0087\n",
      "Epoch [212/1000], Loss: 0.0087\n",
      "Epoch [213/1000], Loss: 0.0087\n",
      "Epoch [214/1000], Loss: 0.0086\n",
      "Epoch [215/1000], Loss: 0.0086\n",
      "Epoch [216/1000], Loss: 0.0086\n",
      "Epoch [217/1000], Loss: 0.0086\n",
      "Epoch [218/1000], Loss: 0.0086\n",
      "Epoch [219/1000], Loss: 0.0086\n",
      "Epoch [220/1000], Loss: 0.0085\n",
      "Epoch [221/1000], Loss: 0.0085\n",
      "Epoch [222/1000], Loss: 0.0085\n",
      "Epoch [223/1000], Loss: 0.0085\n",
      "Epoch [224/1000], Loss: 0.0085\n",
      "Epoch [225/1000], Loss: 0.0085\n",
      "Epoch [226/1000], Loss: 0.0085\n",
      "Epoch [227/1000], Loss: 0.0084\n",
      "Epoch [228/1000], Loss: 0.0084\n",
      "Epoch [229/1000], Loss: 0.0084\n",
      "Epoch [230/1000], Loss: 0.0084\n",
      "Epoch [231/1000], Loss: 0.0084\n",
      "Epoch [232/1000], Loss: 0.0084\n",
      "Epoch [233/1000], Loss: 0.0084\n",
      "Epoch [234/1000], Loss: 0.0083\n",
      "Epoch [235/1000], Loss: 0.0083\n",
      "Epoch [236/1000], Loss: 0.0083\n",
      "Epoch [237/1000], Loss: 0.0083\n",
      "Epoch [238/1000], Loss: 0.0083\n",
      "Epoch [239/1000], Loss: 0.0083\n",
      "Epoch [240/1000], Loss: 0.0083\n",
      "Epoch [241/1000], Loss: 0.0083\n",
      "Epoch [242/1000], Loss: 0.0083\n",
      "Epoch [243/1000], Loss: 0.0082\n",
      "Epoch [244/1000], Loss: 0.0083\n",
      "Epoch [245/1000], Loss: 0.0082\n",
      "Epoch [246/1000], Loss: 0.0082\n",
      "Epoch [247/1000], Loss: 0.0082\n",
      "Epoch [248/1000], Loss: 0.0082\n",
      "Epoch [249/1000], Loss: 0.0082\n",
      "Epoch [250/1000], Loss: 0.0081\n",
      "Epoch [251/1000], Loss: 0.0081\n",
      "Epoch [252/1000], Loss: 0.0081\n",
      "Epoch [253/1000], Loss: 0.0081\n",
      "Epoch [254/1000], Loss: 0.0081\n",
      "Epoch [255/1000], Loss: 0.0081\n",
      "Epoch [256/1000], Loss: 0.0081\n",
      "Epoch [257/1000], Loss: 0.0081\n",
      "Epoch [258/1000], Loss: 0.0081\n",
      "Epoch [259/1000], Loss: 0.0081\n",
      "Epoch [260/1000], Loss: 0.0080\n",
      "Epoch [261/1000], Loss: 0.0080\n",
      "Epoch [262/1000], Loss: 0.0080\n",
      "Epoch [263/1000], Loss: 0.0080\n",
      "Epoch [264/1000], Loss: 0.0080\n",
      "Epoch [265/1000], Loss: 0.0080\n",
      "Epoch [266/1000], Loss: 0.0080\n",
      "Epoch [267/1000], Loss: 0.0080\n",
      "Epoch [268/1000], Loss: 0.0079\n",
      "Epoch [269/1000], Loss: 0.0079\n",
      "Epoch [270/1000], Loss: 0.0079\n",
      "Epoch [271/1000], Loss: 0.0079\n",
      "Epoch [272/1000], Loss: 0.0079\n",
      "Epoch [273/1000], Loss: 0.0079\n",
      "Epoch [274/1000], Loss: 0.0079\n",
      "Epoch [275/1000], Loss: 0.0079\n",
      "Epoch [276/1000], Loss: 0.0079\n",
      "Epoch [277/1000], Loss: 0.0078\n",
      "Epoch [278/1000], Loss: 0.0078\n",
      "Epoch [279/1000], Loss: 0.0078\n",
      "Epoch [280/1000], Loss: 0.0078\n",
      "Epoch [281/1000], Loss: 0.0078\n",
      "Epoch [282/1000], Loss: 0.0078\n",
      "Epoch [283/1000], Loss: 0.0078\n",
      "Epoch [284/1000], Loss: 0.0078\n",
      "Epoch [285/1000], Loss: 0.0078\n",
      "Epoch [286/1000], Loss: 0.0078\n",
      "Epoch [287/1000], Loss: 0.0078\n",
      "Epoch [288/1000], Loss: 0.0078\n",
      "Epoch [289/1000], Loss: 0.0077\n",
      "Epoch [290/1000], Loss: 0.0077\n",
      "Epoch [291/1000], Loss: 0.0077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Запускаем тренировку\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Прогон данных через модель\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Получаем предсказание и обновляем скрытое состояние\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Вычисляем ошибку по всем предсказаниям сразу\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n",
      "File \u001b[1;32mc:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))  \u001b[38;5;66;03m# Преобразуем для Linear слоя\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[1;32mc:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1139\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1139\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1143\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Параметры модели и обучения\n",
    "input_size = len(token_to_idx)  # Размер словаря\n",
    "hidden_size = 128  # Размер скрытого слоя\n",
    "output_size = len(token_to_idx)  # Размер выходного слоя, равен размеру словаря\n",
    "n_layers = 3  # Число GRU слоев\n",
    "num_epochs = 1000  # Число эпох\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = RNN(input_size, hidden_size, output_size, n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Определяем RNN-модель с исправлениями\n",
    "\n",
    "\n",
    "# Функция для тренировки модели\n",
    "def train(model, num_epochs):\n",
    "    model.train()  # Устанавливаем режим обучения\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in generate_chunk():  # Получаем батч из генератора\n",
    "\n",
    "            # Подготовка входных и целевых данных\n",
    "            inputs = torch.LongTensor(batch[:, :-1])  # Все токены, кроме последнего\n",
    "            targets = torch.LongTensor(batch[:, 1:]).reshape(-1)  # Все токены, начиная со второго\n",
    "            \n",
    "            # Инициализация скрытого состояния\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            # Накопление градиентов\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Прогон данных через модель\n",
    "            output, hidden = model(inputs, hidden)  # Получаем предсказание и обновляем скрытое состояние\n",
    "            \n",
    "            # Вычисляем ошибку по всем предсказаниям сразу\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            # Обратное распространение и обновление параметров\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()  # Учитываем среднюю ошибку за батч\n",
    "        \n",
    "        # Выводим информацию о потере после каждой эпохи\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / batch_size:.4f}\")\n",
    "\n",
    "# Запускаем тренировку\n",
    "train(model, num_epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    # Начальная последовательность: токен начала строки + токены seed_phrase\n",
    "    if seed_phrase is not None:\n",
    "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
    "    else: \n",
    "        x_sequence = [token_to_idx['<sos>']]\n",
    "\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
    "    \n",
    "    # Инициализация скрытого состояния\n",
    "    hidden = char_rnn.init_hidden(batch_size=1)\n",
    "\n",
    "    # Генерация начальной последовательности\n",
    "    result = seed_phrase if seed_phrase is not None else \"\"\n",
    "    \n",
    "    # Прогон начальной последовательности через модель\n",
    "    with torch.no_grad():  # Отключаем вычисление градиентов для генерации\n",
    "        for i in range(len(x_sequence[0]) - 1):\n",
    "            _, hidden = char_rnn(x_sequence[:, i:i+1], hidden)\n",
    "    \n",
    "    # Генерация следующих символов\n",
    "    input_token = x_sequence[:, -1]  # Последний токен начальной последовательности\n",
    "    for _ in range(max_length - len(result)):\n",
    "        output, hidden = char_rnn(input_token.unsqueeze(1), hidden)\n",
    "        output = output.squeeze(0) / temperature  # Применяем температуру\n",
    "\n",
    "        # Применяем softmax для получения вероятностей и выбираем следующий токен\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        # Проверка на токен конца строки (например, '<eos>')\n",
    "        if tokens[next_token] == '<eos>':\n",
    "            break\n",
    "\n",
    "        # Добавляем предсказанный символ к результату\n",
    "        result += tokens[next_token]\n",
    "\n",
    "        # Обновляем входной токен для следующего шага\n",
    "        input_token = torch.tensor([next_token], dtype=torch.int64).to(device)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " мой дядя самых честных правил\n",
      "не постещет улечаюцы.\n",
      "тальной полпосенье вутра.\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "\n",
      "и к студет не веняская в тот,\n",
      "стреетсе лидейства пустенье;\n",
      "но как он прогосту залицы,\n",
      "страстенье радетаньех поромный довилный;\n",
      "есто мы может из волский,\n",
      "да ли правневали мои.\n",
      "\n",
      "\n",
      "\n",
      "xxx\n",
      "\n",
      "и споклует их ле затам.\n",
      "все меркливо синной себе,\n",
      "моем их обезореты!\n",
      "козора взор загреной срестки,\n",
      "с из накрачали вден ручяй,\n",
      "\n",
      "\n",
      "\n",
      "viii\n",
      "\n",
      "с идет и стасудь порезь,\n",
      "не безнас рашдет, клонной глененьих,\n",
      "– там цех, разней их нем.\n",
      "\n",
      "\n",
      "\n",
      "xvi\n",
      "\n",
      "б\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
    "\n",
    "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_phrase = ' мой дядя самых честных правил'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# For example:\n",
    "\n",
    "generated_phrases = [\n",
    "     generate_sample(\n",
    "         model,\n",
    "         ' мой дядя самых честных правил',\n",
    "         max_length=500,\n",
    "         temperature=1.\n",
    "     ).replace('<sos>', '')\n",
    "     for _ in range(10)\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "import json\n",
    "if 'generated_phrases' not in locals():\n",
    "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
    "\n",
    "for phrase in generated_phrases:\n",
    "\n",
    "    if not isinstance(phrase, str):\n",
    "        raise ValueError(\"The generated phrase should be a string\")\n",
    "\n",
    "    if len(phrase) != 500:\n",
    "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
    "\n",
    "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
    "    \n",
    "\n",
    "submission_dict = {\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'generated_phrases': generated_phrases\n",
    "}\n",
    "\n",
    "with open('submission_dict.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
